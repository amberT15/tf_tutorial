{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief intro to deep nueral networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://github.com/bharris12/URP_2021_Programming_Course/raw/main/lecture_8/figure/table.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for this tutorial we will focus on supervised learning. More specifically, we will use some classification models to expain the main concepts of deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A deep learning model is constructed by three main parts,model structure, loss function, and optimization.\n",
    "<img src=https://eenews.cdnartwhere.eu/sites/default/files/images/01-picture-library/ChristophHammerschmidt/2016-11-november/Cadence_Hijazi/cnn_fig1.jpeg />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight is adjusted by a process called gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://miro.medium.com/max/1400/1*IfVAzJJWIrCw0saOZAxpzA.png />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic model structure is what we call Multilayer Perceptron\n",
    "<img src=https://miro.medium.com/max/600/1*xxZXeKfVKTRqh54t10815A.jpeg />\n",
    "Its constructed by basic units that are called 'Neurons' or 'Perceptrons'. Each individual one is actually very simply constructed.\n",
    "<img src=https://pythonmachinelearning.pro/wp-content/uploads/2017/09/Single-Perceptron.png.webp />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although each one is a very simple function approximater, by layering them together, the model now has a much greater power and flexibility in approximating functions. But after all, neural networks can be thing of as a function approximator, but with less assumptions.\n",
    "A fun app for visualiztion MLP: https://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP isn't the best fit on all problems. Especially things like images. How do we get 'feature' from images? Per pixel? Thats just way too much. So another kind of layer comes into play.\n",
    "![ChessUrl](https://miro.medium.com/max/1052/0*jLoqqFsO-52KHTn9.gif)\n",
    "\n",
    "We call the weight we have learnt 'filters'. It to some extent represents what pattern the model has learnt to look for. And there are usually multiple filters per convolution layer, give the model the ability to learn more than one pattern.\n",
    "\n",
    "<img src=https://www.researchgate.net/publication/336805909/figure/fig1/AS:817888827023360@1572011300751/Schematic-diagram-of-a-basic-convolutional-neural-network-CNN-architecture-26.ppm />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the above figure structure, pooling layers often exist in deep learning models. Its main usage is to condense information. There are several ways pooling can be done:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://miro.medium.com/max/1400/1*oP-lySI7atBfDgpWQjnB6w.png />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is a widely used deep learning framework. Meaning that all the function/layers that we have mentioned above is already implemented as easy-to-use functions. For this class we will especially depend on TensorFlow.keras.\n",
    "\n",
    "There are lots of classes/blogs online for tensorflow tutorial. Tensorflow itself also come with a few tutorials:\n",
    "https://www.tensorflow.org/tutorials\n",
    "\n",
    "The model that we are going to go through will use real biology data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsQFd76EvhYB"
   },
   "source": [
    "# Convolutional Neural Networks for regulatory genomics\n",
    "\n",
    "In this notebook, we will walk through how to build, train, and evaluate a convolutional neural network (CNN) for the computational task of predicting transcription factor binding sites from Chromatin Immunoprecipitation sequencing (ChIP-seq) data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02bCwLn256nE"
   },
   "outputs": [],
   "source": [
    "import h5py # python module needed to load dataset (stored in an hdf5 file format)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITu7CGCmv_YI"
   },
   "source": [
    "\n",
    "#### Overview\n",
    "Here, we will employ a CNN model to experimental ChIP-seq data for CTCF, a transciption factor that is known to bind to DNA and help form loops. We will set this up as a supervised learning task, specifically a binary classification, where we give the model a sequence and it has to predict whether or not it is associated with TF binding (i.e. presence or absence of a ChIP-seq peak). Thus, we need a set of background sequences which don't have a ChIP-seq peak. \n",
    "\n",
    "________________________________________________________________________________\n",
    "#### How should we choose background sequences?\n",
    "\n",
    "Randomly choosing sequences throughout the genome may include a CTCF motif (the accessibility of DNA is different across cell-types). If the properties of the background sequences are different from the positive sequences (e.g. GC-content differences), then a model may learn this instead of the motif patterns. We could just shuffle the sequences, but we may end up with a model that learns real genomic sequences vs fake genomic sequences. One background sequence that makes sense is to use accessible chromatin in the same cell type BUT does not overlap with the TF binding sites from ChIP-seq.  This way, the background sequences come from regulatory DNA from the same cell type and the only presumable difference between background and positive sequences is the TF binding site.  \n",
    "\n",
    "We already did this and processed the dataset:\n",
    "\n",
    "Let's download the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "A5nREJ71xETA",
    "outputId": "3e60e870-93fd-404b-a3ed-7c2918377324"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/0ks4dk6md2l9mqx/CTCF_200.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC2s_mEgxKaa"
   },
   "source": [
    "For this dataset, the keys for the train sequences is x_train and its corresponding label is y_train.  Note that the transpose changes the shape of the sequences such that the shape is (N,L,A), where N is the number of sequences, L is the length of the sequence=200, and A is the alphabet size=4). \n",
    "\n",
    "We define the alphabet which was used to generate the dataset (dim0=A, dim1=C, dim2=G, dim3=T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "3JID4rQVBs-F",
    "outputId": "5fcd7dd2-bc48-482c-baa9-9b9b3f0ec241"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "file_path = 'CTCF_200.h5'\n",
    "\n",
    "# print shape of training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9p7gNi0jnBUi"
   },
   "source": [
    "There should be 51,156 training sequences each 200 nts long with an alphabet size of 4. Let's see what a sequence actually looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868
    },
    "id": "M1o003KgnBbM",
    "outputId": "122895b1-8e72-474b-deea-8f28b4a92343"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDCanZSMnsrZ"
   },
   "source": [
    "Now let's look at the shape of the training labels (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "AnPEqBTTW5Wr",
    "outputId": "3c61345b-16e1-429e-ff14-b09056612dab"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yh_kuuNtwGLI"
   },
   "source": [
    "# Convolutional Neural Network Overview\n",
    "\n",
    "A convolutional neural network (CNN) is composed of a stack of convolutional modules, each of which perform feature extraction from the previous layer. Each module consists of a convolutional layer, non-linear activation, followed by an optional pooling layer. The convolutional modules are usually followed by one or more dense layers which help it to learn more complex features. The final dense layer in a CNN is where predictions of the model are made. The activations of the output layer depend on the task at hand. In binary classification of TF binding sites, it will be a sigmoid activation (similar to logistic regression).\n",
    "\n",
    "### Convolutional layer\n",
    "\n",
    "Convolutional layers employ a specified number of convolution filters. The output of the convolutional filter scans is called a feature map. Convolutional layers then typically apply a ReLU activation function to the feature maps. Convolutional layers have 4 main hyperparameter choices: number of filters, filter size (i.e. kernel_size), padding, and activation. \n",
    "\n",
    "Eg. Conv1D(filters=24, kernel_size=19, activation='relu', padding='same')\n",
    "\n",
    "- The number of filters specifies how many patterns you aim to capture in that layer.  \n",
    "- The kernel_size sets how large you expect the important patterns to be in the given layer. \n",
    "- The padding argument specifies one of two enumerated values: valid (default) or same. To specify that the output tensor should have the same width and height values as the input tensor, we set padding=same, which instructs TensorFlow to pad the sequences with 0 values at the edges to preserve the shape of the sequence. \n",
    "- The activation sets the non-linearity added after the convolutional scans. The default is linear. We will use rectified linear units (ReLU) here. Other choices include: sigmoid, tanh, elu, softplus, swish, prelu, leaky relu.\n",
    "\n",
    "### Max-pool layer\n",
    "\n",
    "Max-pooling is a technique to downsample the data over a window size set by pool_size. This allows for deeper convolutional layers to learn patterns over a larger sequence context, while significantly reducing the processing time. The strides of max pooling sets how many steps to take for each max-pool operation. If the strides is set to the pool_size, this creates non-overlapping windows of the feature maps. Here, we will downsample the 1st convolutional layer scans by non-overlapping windows of size 25. This will result in a shape of (N, L/25, A).\n",
    "\n",
    "Eg. layers.MaxPool1D(pool_size=25)\n",
    "\n",
    "### fully-connected hidden layer\n",
    "\n",
    "Dense (fully connected) layers, connect every node in the layer (set by num_units) to every node in the preceding layer. The hyperparameters of interest are the number of neurons (units) and the activation, which we will set to relu.\n",
    "\n",
    "layers.Dense(units=64, activation='relu')\n",
    "\n",
    "FYI, most hyperparameter settings for convolutional, max-pooling, and dense layers are chosen through trail-and-error. Most settings result in similar performance -- they are all nearly just as good as each other. Finding the jackpot setting usually means you improve accuracy from 0.9 to 0.92.  \n",
    "\n",
    "\n",
    "### fully-connected output layer\n",
    "\n",
    "The output layer is a linear classifier model. So, the number of predictions we want to make is the number of output neurons we need to create. Here that number is one, because we are only making a single prediction -- whether or not a sequence is associated with a CTCF ChIP-seq peak. If there were more labels, all we have to do is set the 1 to an appropriate value. Note that the activation is dependent on what you are trying to predict. For instance, a regression can be achieved by a simple linear model, without any activation; a binary classificaiton can be achieved with a sigmoid activation, a multi-class prediction can be achieved with a softmax activation. Here we are performing a single-class binary classification, so we will employ sigmoid activations to the output neuron.\n",
    "\n",
    "layers.Dense(units=1, activation='linear')\n",
    "layers.Activation('sigmoid')\n",
    "\n",
    "\n",
    "\n",
    "For more details on CNNs: https://cs231n.github.io/convolutional-networks/\n",
    "Dataset overview\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UySldexXBVp7"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# import modules that will make it easy to build and train the linear regression model\n",
    "\n",
    "\n",
    "# let's setup a sequential model -- which means that the model will be feed \n",
    "# forward (all models in this course will be sequential)\n",
    "\n",
    "\n",
    "# the input layer specifies the shape of the input data to the model. Since our input \n",
    "# features, x, are 1-dimensional, we can set this value to 1. All models require\n",
    "# some input layer (where you feed in data).\n",
    "\n",
    "\n",
    "# a dense connection to 1 unit means that the inputs are fully connected to the \n",
    "# output neuron (in this case a single output neuron). Linear is the default \n",
    "# activation, but here we explicitly write it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmxQ2I0HuvAQ"
   },
   "source": [
    "After we build the model, we can print a summary that shows the order of the layers, the shapes and number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "9jrD8W_eDILw",
    "outputId": "fa9c03c9-702c-4aab-e2e6-fe2d8ef032db"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNLXcm1Ez6Ch"
   },
   "source": [
    "### Build loss function\n",
    "\n",
    "For both training and evaluation, we need to define a loss function that measures how closely the model's predictions match the target classes. This is usually a log-likelihood function for supervised learning. The log-likelihood for a binary classification is the binary cross-entropy loss function:\n",
    "\n",
    "$\\mathcal{L} = \\sum_i y_i \\log(p_i) + (1-y_i)\\log(1-p_i)$\n",
    "\n",
    "Fortunately, Keras makes this super easy as you'll see below.\n",
    "\n",
    "### Build optimizer\n",
    "\n",
    "Now that we have our loss function established, we need to be able to calculate the gradients of the loss function with respect to the parameters of the model, which is efficiently calculated with backpropagation. Then, we can update the weights with gradients scaled by the learning rate, which is known as gradient descent learning. Fortunately, tensorflow/Keras provides a nice wrapper to perform all of this. There are many built in optimizers: sgd, momentum, adam, adagrad, rmsprop. ADAM is the best introductory optimizer for most tasks. It performs an adaptive learning rate based on previous gradients and hence it is much faster to converge and leads to consistently good solutions.  \n",
    "\n",
    "For more info on adam: https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpzhNqaUH5_J"
   },
   "outputs": [],
   "source": [
    "# Now that we built the model, we have to set up the loss function and the optimizer.\n",
    "# Since it's a linear regression, we will set it to 'mean_squared_error'. \n",
    "# we will set the optimizer to stochastic gradient descent 'sgd' (even though this\n",
    "# is unnecessary for a linear regression)\n",
    "\n",
    "\n",
    "\n",
    "# a short cut is: model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auroc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ew1bJMu-0BuF"
   },
   "source": [
    "### Fit the model\n",
    "\n",
    "We want to train our CNN model with mini-batch stochastic gradient descent. So, we need a way to generate mini-batches of data. Keras handles all of this for you through the setting batch_size. It also lets you select the number of epochs to train.  You can also monitor validation performance by adding the validation_data flag.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KNDgTluoIL4B",
    "outputId": "193048dd-691a-427b-b27c-b7642d940ce8"
   },
   "outputs": [],
   "source": [
    "# Now let's fit the model. The main inputs are x, y, epochs, which sets the number \n",
    "# of times to run through the dataset with SGD. Also, we set verbose to false,\n",
    "# because too many outputs are written and it is overwhelming for a linear model.\n",
    "# We will turn this flag to true for other datasets later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tt4MuSUVwx8-"
   },
   "source": [
    "### Evaluate model\n",
    "\n",
    "Evaluating the test performance is easy with Keras. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "shpWSlHkTVml",
    "outputId": "67a1706c-f051-4117-9b05-c38843aed9fe"
   },
   "outputs": [],
   "source": [
    "# evaluate model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6NpcuCZ1iWQ"
   },
   "source": [
    "Now while these saliency plots seem to find a motif pattern that resembles the CTCF motif (see: https://www.factorbook.org/tf/human/CTCF/motif/ENCSR000BIE), the saliency maps are really noisy.  From my experience, this arises because of overfitting, which leads to spurious saliency maps as well -- here overfitting does not affect classification performance on the test set and is termed benign overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redo analysis\n",
    "\n",
    "Let's redo this analysis with some regularization added to our CNN model. A very effective regularization is dropout and early stopping. For convolutional layers, let's add a dropout rate of 0.2 and for dense layers let's use a dropout of 0.5.  For early stopping, there is a built in \"callback\" in keras. Callbacks are objects taht perform actions during training. The early stopping call back checks whether the model's validation performance hasn't improved for x number of epochs. if the x number of epochs exceeds your patience, then training loop is stopped.  See below for example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ilcod74ZSCwy",
    "outputId": "625e1333-4615-4e4a-f625-ec8230385d20"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "# build model\n",
    "\n",
    "\n",
    "# compile model\n",
    "\n",
    "\n",
    "# create a callback for early stopping. Now we can increase the number of epochs \n",
    "# as early stopping will automatically stop training earlier\n",
    "\n",
    "\n",
    "# fit model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGqiA9SWMHsk"
   },
   "source": [
    "Let's evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "FrHQrLIRMBmO",
    "outputId": "fdd1b2c3-2c36-44f7-8ce7-1f5d05fc1d2a"
   },
   "outputs": [],
   "source": [
    "# evaluate model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VONC4Vil0yal"
   },
   "source": [
    "# Validating the model\n",
    "\n",
    "To validate what the network is learning let's calculate a saliency map --  derivative of the output neuron with respect to the inputs. Luckily, tensorflow makes this easy because it calculates gradients using automatic differentiation. We can calculate the gradient with the function model.optimizer.get_gradients. We need to create a keras function to execute this. See code below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJnGwJ1kJYF1"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "# create an op to calculate gradients of outputs with respect to inputs (i.e. saliency map)\n",
    "# Note that we are taking the derivative of the pre-activated output layer (not the output layer)\n",
    "# this is because the sigmoid can make the gradients very small when predictions are close to 1 or 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mdhs8NrR0BgC"
   },
   "source": [
    "Now we should choose which sequences we want to perform saliency analysis on. Let's choose the top 10 test sequences with the highest predictions. To find these, we can first predict the test sequences, sort them in descending order, then slice the top ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y53H52KuNshs"
   },
   "outputs": [],
   "source": [
    "# To get model predictions, we can simply call predict.\n",
    "\n",
    "\n",
    "# sort predictions in decending order\n",
    "\n",
    "\n",
    "# get the top num_plot predictions\n",
    "\n",
    "\n",
    "# calculate saliency maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQwKvDuv5KrI"
   },
   "source": [
    "Let's visualize the saliency maps. We can plot this as a heat map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "GxNSfqM15ShW",
    "outputId": "d48c7fa3-07e8-43b1-8c58-8a5058bda24e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oPkkskO02Xs"
   },
   "source": [
    "This isn't a very nice visualization. Fortunately for you, [Logomaker](https://logomaker.readthedocs.io/en/latest/) (by Tareen and Kinney) can help us visualize saliency maps. We need to first install it on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "f7Uhph4rSaxj",
    "outputId": "8532e5ad-3dde-4dca-ce6b-fdca15df4e07"
   },
   "outputs": [],
   "source": [
    "!pip install logomaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISfd-Wff59Ng"
   },
   "source": [
    "I have created a function to visualize the saliency maps.  You are welcome to take a look at it. It essentially just converts the saliency map in the format needed by logomaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "id": "RZUTCGPPPsXl",
    "outputId": "58d43eb4-3ceb-4ad5-b8d5-cd7dd145ef61"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logomaker \n",
    "\n",
    "def plot_saliency_map(scores, alphabet, ax=None):\n",
    "  L,A = scores.shape\n",
    "  counts_df = pd.DataFrame(data=0.0, columns=list(alphabet), index=list(range(L)))\n",
    "  for a in range(A):\n",
    "    for l in range(L):\n",
    "      counts_df.iloc[l,a] = scores[l,a]\n",
    "\n",
    "  if not ax:\n",
    "    ax = plt.subplot(1,1,1)\n",
    "  logomaker.Logo(counts_df, ax=ax)\n",
    "\n",
    "\n",
    "saliency_scores = (saliency_map * X).numpy()  # grad * input\n",
    "for scores in saliency_scores:\n",
    "  fig = plt.figure(figsize=(10,1))\n",
    "  ax = plt.subplot(1,1,1)\n",
    "  plot_saliency_map(scores, alphabet, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAe5Jw5L1R5V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of 2020_ML_Lecture-CNN_TF_binding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
